from sharedsteps.utils import calculate_algorithm_metrics
import re
import logging
logger = logging.getLogger(__name__)

ALL_VARIANTS = {
                'a': '[aÃÃ¡Ã€Ã Ä‚Äƒáº®áº¯áº°áº±áº´áºµáº²áº³Ã‚Ã¢áº¤áº¥áº¦áº§áºªáº«áº¨áº©ÇÇÃ…Ã¥ÇºÇ»Ã„Ã¤ÇÇŸÃƒÃ£È¦È§Ç Ç¡Ä„Ä…Ä„ÌÄ…ÌÄ„ÌƒÄ…ÌƒÄ€ÄÄ€Ì€ÄÌ€áº¢áº£È€ÈAÌ‹aÌ‹È‚Èƒáº áº¡áº¶áº·áº¬áº­á¸€á¸Èºâ±¥á¶áºšï¼¡ï½@4*]',
                'b': '[bbÌá¸ƒá¸…á¸‡Æ€áµ¬á¶€É“Æƒâ²ƒĞ±Ğ²Î²Æ„â™­É“BBÌá¸‚á¸„á¸†ÉƒÆÆ‚â²‚Ğ‘Ğ’Æ…à¸¿â‚¿â„¬Ê™*]',
                'c': '[cÄ†Ä‡ÄˆÄ‰ÄŒÄÄŠÄ‹Ã‡Ã§á¸ˆá¸‰È»È¼ê’ê“êŸ„ê”Æ‡ÆˆÉ•*]',
                'd': '[dÄÄá¸Šá¸‹á¸á¸‘DÌ¦dÌ¦á¸Œá¸á¸’á¸“á¸á¸ÄÄ‘ÃÃ°Æ‰É–ÆŠÉ—áµ­á¶á¶‘È¡*]',
                'e': '[e3Ğ„Ñ”Ğ•ĞµÃ‰Ã©ÃˆÃ¨Ä”Ä•ÃŠÃªáº¾áº¿á»€á»á»„á»…á»‚á»ƒÃŠÌ„ÃªÌ„ÃŠÌŒÃªÌŒÄšÄ›Ã‹Ã«áº¼áº½Ä–Ä—Ä–ÌÄ—ÌÄ–ÌƒÄ—ÌƒÈ¨È©á¸œá¸Ä˜Ä™Ä˜ÌÄ™ÌÄ˜ÌƒÄ™ÌƒÄ’Ä“á¸–á¸—á¸”á¸•áººáº»È„È…EÌ‹eÌ‹È†È‡áº¸áº¹á»†á»‡á¸˜á¸™á¸šá¸›É†É‡EÌ©eÌ©ÃˆÌ©Ã¨Ì©Ã‰Ì©Ã©Ì©á¶’â±¸ï¼¥ï½…*]',
                'f': '[fá¸ŸÆ’áµ®á¶‚á¸Æ‘*]',
                'g': '[gÇ´ÇµÄÄŸÄœÄÇ¦Ç§Ä Ä¡GÌƒgÌƒÄ¢Ä£á¸ á¸¡Ç¤Ç¥ê ê¡Æ“É á¶ƒï¼§ï½‡qê–ê—ê˜ê™É‹Ê *]',
                'h': '[hÄ¤Ä¥ÈÈŸá¸¦á¸§á¸¢á¸£á¸¨á¸©á¸¤á¸¥á¸ªá¸«HÌ±áº–Ä¦Ä§â±§â±¨êªÉ¦Î—ĞĞ½*]',
                'i': '[iÃÃ­iÌ‡ÌÃŒÃ¬iÌ‡Ì€Ä¬Ä­ÃÃ®ÇÇÃÃ¯á¸®á¸¯Ä¨Ä©iÌ‡ÌƒÄ®Ä¯Ä®ÌÄ¯Ì‡ÌÄ®ÌƒÄ¯Ì‡ÌƒÄªÄ«ÄªÌ€Ä«Ì€á»ˆá»‰ÈˆÈ‰IÌ‹iÌ‹ÈŠÈ‹á»Šá»‹á¸¬á¸­Æ—É¨á¶–Ä°iIÄ±ï¼©ï½‰1lÄºÄ¾Ä¼á¸·á¸¹lÌƒá¸½á¸»Å‚Å€Æšê‰â±¡É«É¬êá¶…É­È´ï¼¬ï½Œ*]',
                'j': '[jÄµÑ˜ÉŸÊÊ²â…‰JÄ´JÌƒĞˆ*]',
                'k': '[ká¸°á¸±Ç¨Ç©Ä¶Ä·á¸²á¸³á¸´á¸µÆ˜Æ™â±©â±ªá¶„ê€êê‚êƒê„ê…ê¢ê£*]',
                'l': '[lÄºá¸¹lÌ…á¸·Ä¾Å‚Å‚Æšê‰lÌ¥Õ¬Î»Â£â„’â„“É¬É«LÄ¹Ä½á¸¸LÌ…á¸¶ÅÈ½Ô¼Î›Â£â„’â„“É®É«*]',
                'm': '[má¸¿á¹á¹ƒmÌ€mÌ‚mÌ„mÌŒÉ±Ğ¼â°¿Î¼á¸¿mÌ€É±mÌ¥á´áµá´¹Má¸¾á¹€á¹‚MÌ€MÌ‚MÌ„MÌŒâ±®Ğœâ°Îœ*]',
                'n': '[nÅƒÅ„Ç¸Ç¹Å‡ÅˆÃ‘Ã±á¹„á¹…Å…Å†á¹†á¹‡á¹Šá¹‹á¹ˆá¹‰NÌˆnÌˆÆÉ²ÅŠÅ‹êê‘ê¤ê¥áµ°á¶‡É³ÈµĞ˜Ğ¸ĞŸĞ¿ï¼®ï½*]',
                'o': '[Ã“Ã³Ã’Ã²ÅÅÃ”Ã´á»á»‘á»’á»“á»–á»—á»”á»•Ç‘Ç’Ã–Ã¶ÈªÈ«ÅÅ‘Ã•Ãµá¹Œá¹á¹á¹È¬È­È®È¯OÍ˜oÍ˜È°È±Ã˜Ã¸Ç¾Ç¿ÇªÇ«Ç¬Ç­ÅŒÅá¹’á¹“á¹á¹‘á»á»ÈŒÈÈÈÆ Æ¡á»šá»›á»œá»á» á»¡á»á»Ÿá»¢á»£á»Œá»á»˜á»™OÌ©oÌ©Ã’Ì©Ã²Ì©Ã“Ì©Ã³Ì©ÆŸÉµêŠê‹êŒêâ±ºï¼¯ï½0*]',
                'p': '[pê“Æ¥á¹•á¹—pÌ„ê‘â±‚Ï€â„—Ñ€Æ¿Ã¾Ãáš¦Pê’Æ¤á¹”á¹–PÌ„êâ°’Î Â¶â‚±â„—â„™Î¡Ğ á¢Ç·*]',
                'q': '[qê–ê—ê˜ê™É‹Ê ï¼±ï½‘*]',
                'r': '[rÅ”Å•Å˜Å™á¹˜á¹™Å–Å—ÈÈ‘È’È“á¹šá¹›á¹œá¹á¹á¹ŸRÌƒrÌƒÉŒÉê¦ê§â±¤É½áµ²á¶‰*]',
                's': '[sÅšÅ›á¹¤á¹¥ÅœÅÅ Å¡á¹¦á¹§á¹ á¹¡ÅÅŸá¹¢á¹£á¹¨á¹©È˜È™SÌ©sÌ©ê¨ê©â±¾È¿Ê‚á¶Šáµ´*]',
                't': '[tÅ¤Å¥á¹ªá¹«Å¢Å£á¹¬á¹­ÈšÈ›á¹°á¹±á¹®á¹¯Å¦Å§È¾â±¦Æ¬Æ­Æ®ÊˆTÌˆáº—áµµÆ«È¶*]',
                'u': '[uÅ«ÃºÇ”Ã¹Å­Ã»Ã¼Å¯Å³Å©Å±È•Å«á¹³á¹µá¹·á¹¹á¹»Ç–Ç˜ÇœÇšá»§Å©á»¥Æ°á»«á»­á»¯á»©á»±Ï…ÏÏ‹Î°á½á½”á½’á½–á½‘á½•á½“á½—á½ºá¿¦á¿ á¿¡á¿¢á¿§Ê‰ÊŠï½•UÅªÇ“Ã™Å¬Ã›ÃœÅ®Å²Å¨Å°È”á¹²á¹´á¹¶á¹¸á¹ºÇ•Ç—Ç™Ç›á»¦Å¨á»¤Æ¯á»ªá»¬á»®á»¨á»°Ô±Õ„ÕÉ„Æ±ï¼µ*]',
                'v': '[Vá¹¼á¹¾VÌ‡êVÌ…Ñ´Æ²É…ï¼¶*]',
                'w': '[wáºƒáºÅµáº˜áº…áº‡áº‰ÊÊ·Ê¬Ôï½—Ï‰Ñ¡Wáº‚áº€Å´áº„áº†áºˆÔœâ‚©×©Ğ¨Ğ©Ñ *]',
                'x': '[xáºáº‹á¶xÌ‚xÌ„xÌ±Ñ…Ò³Ó½Ó¿á•½Ã—âœ–â¨¯âœ—âœ˜â˜’â˜“ğ„ªğ•©XáºŒáºŠXÌ‚XÌ„XÌ²Î§Ï‡ÍµÏ‡Î§âœ•â•³â¨‰ğ•ã„¨ãƒ¡ä¹‚ã…áš·áš¸*]',
                'y': '[yÃÃ½á»²á»³Å¶Å·YÌŠáº™Å¸Ã¿á»¸á»¹áºáºÈ²È³á»¶á»·á»´á»µÉÉÆ³Æ´á»¾á»¿*]',
                'z': '[zÅ¾Å¼áº“Åºáº•áº‘ÊÊ‘È¥Æ¶â²Ê‘ÊÊ’Æ¨ZÅ½Å»áº’áº”áºÅ¹ÆµÎ–Î¶Î–â²Œâ„¤â˜¡ä¹™ä¹‹Æ§*]'
            }


class TreesFilter:

    @classmethod
    def train(cls, system, **kwargs):
        rules = system.read_rules()
        if len(rules) == 0:
            return False, "No rules found for the participant"
        
        tree_filter = TreesFilter(rules)
        return True, tree_filter

    def __init__(self, rules, debug=False):
        self.debug = debug 
        self.rules = rules
        # sorted by priority from small to large
        self.rules = sorted(self.rules, key=lambda x: x["priority"])
    
    def _build_word_regex(self, word, variants):
        """
            build a regex for a word
            @param word: a word
        """
        regex = r"\b(?=\w)" # matching the beginning of a word
        for char in word:
            if variants and (char.lower() in ALL_VARIANTS):
                regex += ALL_VARIANTS[char.lower()] + "+"
            else:
                regex += char
        regex += r"\b(?!\w)" # matching the end of a word
        return regex
    
    def _build_statement(self, unit, variants=False):
        regex_list = [self._build_word_regex(word, variants) for word in unit["words"]]
        or_regex = "|".join(regex_list) # a regex that checks if any of the words in the list is in the text
        def func(text):
            match = re.search(or_regex, text, re.IGNORECASE)
            if match:
                return True, match.group()  # Return True and the matched pattern
            else:
                return False, None  # Return False and None if no match
        return func

                
    def _build_rule_function(self, rule):
        units = rule["units"]
        variants = rule["variants"]
        funcs = [(unit["type"], self._build_statement(unit, variants)) for unit in units]
        def rule_function(text):
            patterns = []
            for type, func in funcs:
                result, pattern = func(text)
                if result:
                    patterns.append([type, pattern])

                if type == "include" and not result:
                    return False, patterns
                if type == "exclude" and result:
                    return False, patterns
            return True, patterns
        return rule_function

    def _test_rule(self, rule, dataset):
        rule_function = self._build_rule_function(rule)
        predictions = {}
        for index in range(len(dataset)):
            pred, patterns = rule_function(dataset[index])
            predictions[index] = {
                "pred": pred,
                "patterns": patterns
            }
        return predictions

    def test_model(self, X, y=None):
        """
            There is no training stage for Trees Filter. We only test the model against X, y
            Here, in order to generate explanations for each prediction, 
            instead of building a model with all rules, we generate predictions for each rule individually and aggregate the results for the final prediction.

            @param X: a list of texts
            @param y: a list of 0, 1 representing the labels of the texts

        """
        X_test, y_test = X, y

        
        texts_predictions = [[] for _ in range(len(X_test))]

        for index in range(len(self.rules)):
            rule = self.rules[index]
            rule_id = rule["id"]
            rule_pred = self._test_rule(rule, X_test)
            
            if rule_pred is not None:
                for index in range(len(X_test)):
                    # the difference between None and 0/1 is still important for the frontend to display, even though None predictions are treated as 0 in the classifier
                    texts_predictions[index].append({
                        "id": rule_id,
                        "prediction": rule["action"] if rule_pred[index]["pred"] else None,
                        "patterns": rule_pred[index]["patterns"]
                    })
            
        prediction = [None for _ in range(len(X_test))] # overall predictions
        for index in range(len(X_test)):
            text_pred = texts_predictions[index]
            """ 
                we have already sorted the rules by priority in the constructor
                use the action of the first rule has a True prediction and the highest priority as the final prediction
            """
            for pred in text_pred:
                if pred["prediction"] is not None:
                    prediction[index] = pred["prediction"]
                    break
            
        # we still make sure that the backend operates with a priority framework but we return the result in a simplified way.
        prediction = [(0 if pred is None else pred) for pred in prediction]  
        
        # if the user builds the model interactively, then y_test will be None
        if y_test is not None:
            # we treat None (not affected texts) as approved texts, which is 0
            performance = calculate_algorithm_metrics(y_test, prediction)
        else:
            performance = {}

        return {
            "prediction": prediction,
            "texts_predictions": texts_predictions,
            "performance": performance
        }